{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "This notebook was provided by Udacity as example.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "[![](https://trello-attachments.s3.amazonaws.com/5d93d97e715aba7c97fde8f7/5e9e457961d16e8a352b67c9/da4327f262b337b51a85826ca5e5cf6f/image.png)](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)\n",
    "\n",
    "---\n",
    "\n",
    "# Index\n",
    "\n",
    "- [1. Setup the Environment](#1.-Setup-the-Environment)\n",
    "- [2. Start the Environment](#2.-Start-the-Environment)\n",
    "- [3. Examine the State and Action Spaces](#3.-Examine-the-State-and-Action-Spaces)\n",
    "- [4. Develop the Agent to collect bananas](#4.-Develop-the-Agent-to-collect-bananas)\n",
    " - [4.1 Define trainer function](#4.1-Define-trainer-function)\n",
    " - [4.2 Training agent on environment](#4.2.-Training-agent-on-environment)\n",
    " - [4.3 Plot agent losses along the training session](#4.2-Plot-agent-losses-along-the-training-session)\n",
    " - [4.4 Save weights](#4.4-Save-weights)\n",
    " - [4.5 Load weights](#4.5-Load-weights)\n",
    "- [5. Test Agent on Environment](#5.-Test-Agent-on-Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the Environment\n",
    "\n",
    "<img align=\"left\" width=\"150\" src=\"https://www.nclouds.com/img/services/toolkit/sagemaker.png\"/>\n",
    "\n",
    "This notebook was developed on AWS SageMaker.\n",
    "\n",
    "The kernel used is **conda_python3**\n",
    "\n",
    "To setup this environment on SageMaker you need to run the next 2 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `env_path` variable to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Linux** (x86_64): `\"/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86_64, headless): `\"/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "If your system isn't in the list, please, download the environment from here.\n",
    "\n",
    "The `file_name` parameter needs to be something like that:\n",
    "\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana_Linux_NoVis/Banana.x86_64\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = \"Banana_Linux_NoVis/Banana.x86_64\"\n",
    "#env_path = \"Banana_Linux/Banana.x86_64\"\n",
    "\n",
    "env = UnityEnvironment(file_name=env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Develop the Agent to collect bananas\n",
    "\n",
    "In the next code cells, we will train the Agent to work on environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_trainer(agent, n_episodes=100, print_range=10, eps_start=1.0, eps_end=0.01, eps_decay=0.995, early_stop=13):\n",
    "    \"\"\"Deep Q-Learning trainer.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        print_range (int): range to print partials results\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        early_stop (int): Stop training when achieve a defined score respecting 10 min n_episodes.\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=print_range)  # last 100 scores\n",
    "    scores_mean = []\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0] \n",
    "            agent.step(state, action, reward, next_state, done, i)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(scores_window)), end=\"\")\n",
    "        if i % print_range == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window) >= early_stop and i > 10:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(scores_window)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training agent on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import agent\n",
    "importlib.reload(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agent = agent.Agent(state_size, action_size, seed=1412, nb_hidden=128, learning_rate=5.0e-4,\n",
    "              memory_size=int(3e4), prioritized_memory=True, batch_size=64,\n",
    "              gamma=0.99, tau=0.001, small_eps=1e-5, update_every=5)\n",
    "\n",
    "scores = dqn_trainer(agent, n_episodes=100)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Plot agent losses along the training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"checkpoint.pth\"\n",
    "agent.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"checkpoint.pth\"\n",
    "agent.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test Agent on Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = agent.act(state, eps)\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    score += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
