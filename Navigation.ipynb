{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "This notebook was provided by Udacity as example.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "[![](https://trello-attachments.s3.amazonaws.com/5d93d97e715aba7c97fde8f7/5e9e457961d16e8a352b67c9/da4327f262b337b51a85826ca5e5cf6f/image.png)](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)\n",
    "\n",
    "---\n",
    "\n",
    "# Index\n",
    "\n",
    "- [1. Setup the Environment](#1.-Setup-the-Environment)\n",
    "- [2. Start the Environment](#2.-Start-the-Environment)\n",
    "- [3. Examine the State and Action Spaces](#3.-Examine-the-State-and-Action-Spaces)\n",
    "- [4. Develop the Agent to collect bananas](#4.-Develop-the-Agent-to-collect-bananas)\n",
    " - [4.1 Define trainer function](#4.1-Define-trainer-function)\n",
    " - [4.2 Training agent on environment](#4.2.-Training-agent-on-environment)\n",
    " - [4.3 Plot agent losses along the training session](#4.2-Plot-agent-losses-along-the-training-session)\n",
    " - [4.4 Save weights](#4.4-Save-weights)\n",
    " - [4.5 Load weights](#4.5-Load-weights)\n",
    "- [5. Test Agent on Environment](#5.-Test-Agent-on-Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the Environment\n",
    "\n",
    "<img align=\"left\" width=\"150\" src=\"https://www.nclouds.com/img/services/toolkit/sagemaker.png\"/>\n",
    "\n",
    "This notebook was developed on AWS SageMaker.\n",
    "\n",
    "The kernel used is **conda_python3**\n",
    "\n",
    "To setup this environment on SageMaker you need to run the next 2 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from agent import Agent\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `env_path` variable to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Linux** (x86_64): `\"/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86_64, headless): `\"/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "If your system isn't in the list, please, download the environment from here.\n",
    "\n",
    "The `file_name` parameter needs to be something like that:\n",
    "\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana_Linux_NoVis/Banana.x86_64\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = \"Banana_Linux_NoVis/Banana.x86_64\"\n",
    "#env_path = \"Banana_Windows_x86_64/Banana.exe\"\n",
    "\n",
    "env = UnityEnvironment(file_name=env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define helper functions to Agent trainment\n",
    "\n",
    "Here we will create two functions that will be very helpful to train the agent.\n",
    "\n",
    "First one is `dqn_trainer()` that will train the agent in the environment.\n",
    "\n",
    "Last one is `GridSearch()` that will create all possible combinations from parameter's arrays.\n",
    "For each time we will save the results on a json log file, it's very helpful to split training time if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_trainer(agent, n_episodes=100, print_range=10, eps_start=1.0, eps_end=0.01, eps_decay=0.995, early_stop=13, verbose=False):\n",
    "    \"\"\"Deep Q-Learning trainer.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        print_range (int): range to print partials results\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        early_stop (int): Stop training when achieve a defined score respecting 10 min n_episodes.\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=print_range)  # last 100 scores\n",
    "    scores_mean = []\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            try:\n",
    "                env_info = env.step(action)[brain_name]\n",
    "            except:\n",
    "                print(env.step(action))\n",
    "                break\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0] \n",
    "            agent.step(state, action, reward, next_state, done, i)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        if verbose:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(scores_window)), end=\"\")\n",
    "            if i % print_range == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window) >= early_stop and i > 10:\n",
    "            if verbose:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(scores_window)))\n",
    "            break\n",
    "            \n",
    "    return scores, i, np.mean(scores_window)\n",
    "\n",
    "def GridSearch(param_grid, n_episodes=200, log_path='gridsearch_logs.json', verbose=False):\n",
    "    \"\"\"GridSearch for agent trainer.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        param_grid (dict): All parameters available for Agent class\n",
    "        n_episodes (int): The number of episodes that each agent will be trained.\n",
    "        log_path (str): The JSON file to save parameters.\n",
    "        verbose (bool): print some information on console\n",
    "    \"\"\"\n",
    "    if not isinstance(log_path, str) or \".json\" not in log_path:\n",
    "        raise ValueError('log_path need to be a string with \".json\" in the end.')\n",
    "        \n",
    "    try:\n",
    "        with open(log_path, 'r') as outfile:\n",
    "            param_grid = json.load(outfile)\n",
    "        if verbose:\n",
    "            print(\"param_grid was imported from file: {}\".format(log_path))\n",
    "    except FileNotFoundError as err:\n",
    "        param_grid = list(ParameterGrid(param_grid))\n",
    "        param_grid = [{\"params\": each} for each in param_grid]\n",
    "        with open(log_path, 'w') as outfile:\n",
    "            json.dump(param_grid, outfile)\n",
    "        if verbose:\n",
    "            print(\"No log file available. We created ParameterGrid and save it on file: {}\".format(log_path))\n",
    "    if verbose:\n",
    "        print(\"The param_grid will result in {} fits.\".format(len(param_grid)))\n",
    "        time.sleep(0.3)\n",
    "               \n",
    "    with tqdm(total=len(param_grid)) as pbar:\n",
    "        for i in range(len(param_grid)):\n",
    "            if param_grid[i].get(\"avg_score\") == None: \n",
    "                start_time = datetime.now()\n",
    "                env_info = env.reset(train_mode=True)[brain_name]\n",
    "                agent = Agent(state_size=param_grid[i][\"params\"][\"state_size\"],\n",
    "                              action_size=param_grid[i][\"params\"][\"action_size\"],\n",
    "                              seed=param_grid[i][\"params\"][\"seed\"],\n",
    "                              nb_hidden=param_grid[i][\"params\"][\"nb_hidden\"],\n",
    "                              learning_rate=param_grid[i][\"params\"][\"learning_rate\"],\n",
    "                              memory_size=param_grid[i][\"params\"][\"memory_size\"],\n",
    "                              prioritized_memory=param_grid[i][\"params\"][\"prioritized_memory\"],\n",
    "                              batch_size=param_grid[i][\"params\"][\"batch_size\"],\n",
    "                              gamma=param_grid[i][\"params\"][\"gamma\"],\n",
    "                              tau=param_grid[i][\"params\"][\"tau\"],\n",
    "                              small_eps=param_grid[i][\"params\"][\"small_eps\"],\n",
    "                              update_every=param_grid[i][\"params\"][\"update_every\"])\n",
    "\n",
    "                score, episodes, last_avg_score = dqn_trainer(agent, n_episodes=n_episodes, verbose=False)\n",
    "                training_time = datetime.now() - start_time\n",
    "                \n",
    "                param_grid[i][\"avg_score\"] = last_avg_score\n",
    "                param_grid[i][\"training_time\"] = str(training_time).split('.')[0] \n",
    "                param_grid[i][\"score_history\"] = score\n",
    "                param_grid[i][\"episodes\"] = episodes\n",
    "                with open(log_path, 'w') as outfile:\n",
    "                    json.dump(param_grid, outfile)\n",
    "            pbar.update(1)\n",
    "            \n",
    "    best_params = {}\n",
    "    best_params[\"episodes\"] = 999 # random best_params\n",
    "    \n",
    "    for each in param_grid:\n",
    "        if each[\"episodes\"] < best_params[\"episodes\"]:\n",
    "            best_params = each\n",
    "        \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Develop the Agent to collect bananas\n",
    "\n",
    "In the next code cells, we will train the Agent to work on environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Test parameters with GridSearch\n",
    "\n",
    "First of all, we will apply the `GridSearch()` funtion to find candidates for best parameters. In this first step, our parameters will very different of each other.\n",
    "\n",
    "In the second part, we will try to refine our parameters a little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"state_size\": [state_size],\n",
    "    \"action_size\": [action_size],\n",
    "    \"seed\": [199],\n",
    "    \"nb_hidden\": [(256, 128), (128, 128), (64, 64)],\n",
    "    \"learning_rate\": [1e-5, 1e-4, 1e-3],\n",
    "    \"memory_size\": [int(1e5), int(1e4)],\n",
    "    \"prioritized_memory\": [False, True],\n",
    "    \"batch_size\": [64, 32],\n",
    "    \"gamma\": [0.99, 0.95, 0.9, 0.87],\n",
    "    \"tau\": [1e-2, 1e-3, 1e-4],\n",
    "    \"small_eps\": [1e-2, 1e-3, 1e-4],\n",
    "    \"update_every\": [4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "best_params = GridSearch(params, n_episodes=200, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Training agent with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "agent = Agent(state_size, action_size, seed=199, nb_hidden=(256, 128), learning_rate=3e-4,\n",
    "              memory_size=int(1e5), prioritized_memory=False, batch_size=64,\n",
    "              gamma=0.9, tau=1e-3, small_eps=1e-2, update_every=4)\n",
    "\n",
    "scores, episodes, last_avg_score = dqn_trainer(agent, n_episodes=100, verbose=True)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Plot agent losses along the training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.losses)\n",
    "plt.title(\"Losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model.pt\"\n",
    "agent.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model.pt\"\n",
    "agent.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test Agent on Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n",
    "eps = 0.05\n",
    "while True:\n",
    "    action = agent.act(state, eps)\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    score += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
